For my web crawler, I decided to use Apache Lucene as my tool for indexing keywords and storing documents linking to urls. My logic happens in 4 Java classes, MainCrawler.java, Parser.java, MongoDBMS.java, and Data.java. They can be found in src/main/java/org/example. I compiled my project as a Maven project. My code would not run on other devices because one, I import many local .jar files which are necessary to use apache lucene (which can be found in my pom.xml file), and also because I use my own MongoDB database which I would like to keep secure. I first started by defining my seed url, which I picked to be https://gloutir.com. I would then except as a command line argument a “-words” parameter, which indicated the maximum number of keywords allowed to be extracted and indexed per url that I crawled. I then started would call startParsing() to begin my crawling.

startParsing() worked by having a while loop that ran until the counter of urls parsed had reached a certain threshold (1050 in my case). Each loop iteration the loop would print out the new url it was parsing, create a document out of it, and add that document to the lucene index writer so the keywords can be indexed. Finally the writer was closed and data was written to text files so I could later represent on excel graphs.

I used a MongoDB database to store 2 collections, one collection being the URLs I had crawled, and the other being the URLs on my To Be Crawled list. As you can see, by the time my crawler finished running there were 1,050 urls on the searched list, while there were 12,672 urls on the To Be Searched list. I used the djb2 hash algorithm to provide hashes for my urls which serve as the item “_id” for each url, and I use this to check that no website I try to crawl I have already crawled. I excluded my Mongo connection string in my code for privacy purposes.

My getDocument() function would create the document to be indexed by Lucene. I would first try to connect to the url, and if that failed I would simply skip the url and continue to the next one, logging that I skipped a url. I used Jsoup to parse my website and scrape all the data I needed from it. I would then add 4 fields to my document: the path (url), title of the page, hashed url (for my mongodb database), and notable keywords which I will explain how I get next. I then scan the page for urls and add them to my to be searched list in mongodb, and I finally add the url that I just searched to my searched list in mongodb.

My getKeywords() function would get notable keywords from the body of text from the html page. I used an opensource API called rapidrake which counted the frequency of each phrase in the text. I then sorted those words with most frequent being at the top, and then built a string of the keywords to be added as an index, while making sure to not go over the limit of the maximum number of words I can have indexed per page (I specify that in the command line).

I have a Data class that collected my data for me to display with graphs. After each page had been crawled, I set numerous statistics for each page in a list a datapoints, storing useful information like timestamp, total urls crawled at this point, number of urls to crawl at this point, number of urls to search and keywords added from this url, the updated ratio of crawled to not crawled urls, and also updating the number of pages per minute each time a minute had passed. At the end of my program, I exported each of these fields into respective text files which you can view at the root level of my directory: timestamp.txt, totalUrlsCrawled.txt, urlsAddedToSearch.txt, totalUrlsToSearch.txt, numAddedKeywords.txt, totalKeywordsExtracted.txt, crawlToBeCrawled.txt, and pagesPerMin.txt.

I have my lucene database stored in a directory called “index-directory”, where each keyword I extracted is labeled and the document it corresponds to is also labeled in a list format. Lucene uses an inverted index format, where each keyword stores a list of the documents that include it. This allows for easy querying and indexing. I chose to have my database encoded in a readable format so I can easily check that what I am indexing and storing is correct.

My command line outputs essentially just displayed each time a new url was being crawled/parsed/indexed, and once it was finished that it was added to the writer which meant that the page was properly scanned and indexed into the database. Occasionally, if some error were reached on the page and not all of the information was able to be obtained, then my crawler would simply skip the URL and continue on to the next one, ignoring any data that may have been accumulated from the uncrawlable url.
